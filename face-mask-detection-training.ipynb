{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.026259,
     "end_time": "2021-06-21T14:15:13.243898",
     "exception": false,
     "start_time": "2021-06-21T14:15:13.217639",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-21T14:15:13.300565Z",
     "iopub.status.busy": "2021-06-21T14:15:13.298602Z",
     "iopub.status.idle": "2021-06-21T14:15:20.211441Z",
     "shell.execute_reply": "2021-06-21T14:15:20.210703Z",
     "shell.execute_reply.started": "2021-06-21T13:25:59.933233Z"
    },
    "papermill": {
     "duration": 6.943102,
     "end_time": "2021-06-21T14:15:20.211626",
     "exception": false,
     "start_time": "2021-06-21T14:15:13.268524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "import cv2\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-21T14:15:20.265287Z",
     "iopub.status.busy": "2021-06-21T14:15:20.264512Z",
     "iopub.status.idle": "2021-06-21T14:15:20.270002Z",
     "shell.execute_reply": "2021-06-21T14:15:20.269351Z",
     "shell.execute_reply.started": "2021-06-21T13:26:06.898406Z"
    },
    "papermill": {
     "duration": 0.033568,
     "end_time": "2021-06-21T14:15:20.270178",
     "exception": false,
     "start_time": "2021-06-21T14:15:20.236610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask_path = \"../input/face-mask-12k-images-dataset/Face Mask Dataset/Train/WithMask\"\n",
    "no_mask_path = \"../input/human-faces/Humans\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-21T14:15:20.322809Z",
     "iopub.status.busy": "2021-06-21T14:15:20.321966Z",
     "iopub.status.idle": "2021-06-21T14:15:20.470531Z",
     "shell.execute_reply": "2021-06-21T14:15:20.469985Z",
     "shell.execute_reply.started": "2021-06-21T13:26:06.911104Z"
    },
    "papermill": {
     "duration": 0.176757,
     "end_time": "2021-06-21T14:15:20.470667",
     "exception": false,
     "start_time": "2021-06-21T14:15:20.293910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_mask = []\n",
    "target_mask = []\n",
    "for i in os.listdir(mask_path):\n",
    "    pic = os.path.join(mask_path + \"/\", i)\n",
    "    image_mask.append(pic)\n",
    "    target_mask.append(1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-21T14:15:20.524798Z",
     "iopub.status.busy": "2021-06-21T14:15:20.523922Z",
     "iopub.status.idle": "2021-06-21T14:15:20.859899Z",
     "shell.execute_reply": "2021-06-21T14:15:20.860508Z",
     "shell.execute_reply.started": "2021-06-21T13:26:07.061998Z"
    },
    "papermill": {
     "duration": 0.365739,
     "end_time": "2021-06-21T14:15:20.860792",
     "exception": false,
     "start_time": "2021-06-21T14:15:20.495053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_no_mask = []\n",
    "target_no_mask = []\n",
    "for i in os.listdir(no_mask_path):\n",
    "    pic = os.path.join(no_mask_path + \"/\", i)\n",
    "    image_no_mask.append(pic)\n",
    "    target_no_mask.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-21T14:15:20.931248Z",
     "iopub.status.busy": "2021-06-21T14:15:20.930512Z",
     "iopub.status.idle": "2021-06-21T14:15:20.933814Z",
     "shell.execute_reply": "2021-06-21T14:15:20.934291Z",
     "shell.execute_reply.started": "2021-06-21T13:26:07.503984Z"
    },
    "papermill": {
     "duration": 0.047938,
     "end_time": "2021-06-21T14:15:20.934447",
     "exception": false,
     "start_time": "2021-06-21T14:15:20.886509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask = pd.DataFrame()\n",
    "mask[\"image\"] = image_mask\n",
    "mask[\"target\"] = target_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-21T14:15:20.997091Z",
     "iopub.status.busy": "2021-06-21T14:15:20.994881Z",
     "iopub.status.idle": "2021-06-21T14:15:20.998007Z",
     "shell.execute_reply": "2021-06-21T14:15:20.998561Z",
     "shell.execute_reply.started": "2021-06-21T13:26:07.525978Z"
    },
    "papermill": {
     "duration": 0.039747,
     "end_time": "2021-06-21T14:15:20.998744",
     "exception": false,
     "start_time": "2021-06-21T14:15:20.958997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "no_mask = pd.DataFrame()\n",
    "no_mask[\"image\"] = image_no_mask\n",
    "no_mask[\"target\"] = target_no_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-21T14:15:21.056526Z",
     "iopub.status.busy": "2021-06-21T14:15:21.055719Z",
     "iopub.status.idle": "2021-06-21T14:15:21.079352Z",
     "shell.execute_reply": "2021-06-21T14:15:21.079868Z",
     "shell.execute_reply.started": "2021-06-21T13:26:07.546296Z"
    },
    "papermill": {
     "duration": 0.055818,
     "end_time": "2021-06-21T14:15:21.080039",
     "exception": false,
     "start_time": "2021-06-21T14:15:21.024221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4497</th>\n",
       "      <td>../input/face-mask-12k-images-dataset/Face Mas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12026</th>\n",
       "      <td>../input/human-faces/Humans/1 (1648).jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9452</th>\n",
       "      <td>../input/human-faces/Humans/1 (2821).jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5775</th>\n",
       "      <td>../input/human-faces/Humans/1 (5535).jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5108</th>\n",
       "      <td>../input/human-faces/Humans/1 (5716).jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5320</th>\n",
       "      <td>../input/human-faces/Humans/1 (886).jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4043</th>\n",
       "      <td>../input/face-mask-12k-images-dataset/Face Mas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7025</th>\n",
       "      <td>../input/human-faces/Humans/1 (109).png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4070</th>\n",
       "      <td>../input/face-mask-12k-images-dataset/Face Mas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12071</th>\n",
       "      <td>../input/human-faces/Humans/1 (192).jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12219 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   image  target\n",
       "4497   ../input/face-mask-12k-images-dataset/Face Mas...       1\n",
       "12026           ../input/human-faces/Humans/1 (1648).jpg       0\n",
       "9452            ../input/human-faces/Humans/1 (2821).jpg       0\n",
       "5775            ../input/human-faces/Humans/1 (5535).jpg       0\n",
       "5108            ../input/human-faces/Humans/1 (5716).jpg       0\n",
       "...                                                  ...     ...\n",
       "5320             ../input/human-faces/Humans/1 (886).jpg       0\n",
       "4043   ../input/face-mask-12k-images-dataset/Face Mas...       1\n",
       "7025             ../input/human-faces/Humans/1 (109).png       0\n",
       "4070   ../input/face-mask-12k-images-dataset/Face Mas...       1\n",
       "12071            ../input/human-faces/Humans/1 (192).jpg       0\n",
       "\n",
       "[12219 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([mask, no_mask], axis=0, ignore_index=True)\n",
    "data = shuffle(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-21T14:15:21.140197Z",
     "iopub.status.busy": "2021-06-21T14:15:21.139421Z",
     "iopub.status.idle": "2021-06-21T14:15:22.828146Z",
     "shell.execute_reply": "2021-06-21T14:15:22.829546Z",
     "shell.execute_reply.started": "2021-06-21T13:26:07.589662Z"
    },
    "papermill": {
     "duration": 1.724726,
     "end_time": "2021-06-21T14:15:22.829839",
     "exception": false,
     "start_time": "2021-06-21T14:15:21.105113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "class dataset:\n",
    "    def __init__(self, image_path, targets, resize = None, augmentation = None):\n",
    "        \n",
    "        self.image_path = image_path\n",
    "        self.targets = targets\n",
    "        self.resize = resize\n",
    "        self.augmentation = augmentation\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.image_path)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \n",
    "        image = Image.open(self.image_path[item])\n",
    "        \n",
    "        image = image.convert(\"RGB\")\n",
    "        \n",
    "        target = self.targets[item]\n",
    "        \n",
    "        if self.resize is not None:\n",
    "            image = image.resize((\n",
    "                self.resize[1], self.resize[0]\n",
    "            ),\n",
    "            resample = Image.BILINEAR\n",
    "            )\n",
    "            \n",
    "        image = np.array(image)\n",
    "        \n",
    "        if self.augmentation is not None:\n",
    "            augmented = self.augmentation(image=image)\n",
    "            image = augmented[\"image\"]\n",
    "            \n",
    "            \n",
    "        image = np.transpose(image, (2,0,1)).astype(np.float32)\n",
    "            \n",
    "        return {\n",
    "            \"image\" : torch.tensor(image, dtype= torch.float),\n",
    "            \"target\" : torch.tensor(target, dtype= torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-21T14:15:35.079087Z",
     "iopub.status.busy": "2021-06-21T14:15:35.077974Z",
     "iopub.status.idle": "2021-06-21T14:15:35.081295Z",
     "shell.execute_reply": "2021-06-21T14:15:35.080730Z",
     "shell.execute_reply.started": "2021-06-21T13:26:20.564511Z"
    },
    "papermill": {
     "duration": 0.043574,
     "end_time": "2021-06-21T14:15:35.081416",
     "exception": false,
     "start_time": "2021-06-21T14:15:35.037842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#engine\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def train(data_loader, model, optimizer, device):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for data in data_loader:\n",
    "        inputs = data[\"image\"]\n",
    "        targets = data[\"target\"]\n",
    "        \n",
    "        inputs = inputs.to(device, dtype = torch.float)\n",
    "        targets = targets.to(device, dtype= torch.float)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(inputs)\n",
    "       \n",
    "        loss = nn.BCEWithLogitsLoss()(output, targets.view(-1,1))\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "def evaluation(data_loader, model, device):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    final_targets = []\n",
    "    final_outputs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for data in data_loader:\n",
    "            \n",
    "            inputs = data[\"image\"]\n",
    "            targets = data[\"target\"]\n",
    "            \n",
    "            inputs = inputs.to(device, dtype= torch.float)\n",
    "            targets = targets.to(device, dtype = torch.float)\n",
    "            \n",
    "            output = model(inputs)\n",
    "            \n",
    "            targets = targets.detach().cpu().numpy().tolist()\n",
    "            output = output.detach().cpu().numpy().tolist()\n",
    "            \n",
    "            final_targets.extend(targets)\n",
    "            final_outputs.extend(output)\n",
    "            \n",
    "    return final_outputs, final_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-21T14:15:35.148641Z",
     "iopub.status.busy": "2021-06-21T14:15:35.147034Z",
     "iopub.status.idle": "2021-06-21T14:15:36.560539Z",
     "shell.execute_reply": "2021-06-21T14:15:36.562085Z",
     "shell.execute_reply.started": "2021-06-21T13:33:56.125948Z"
    },
    "papermill": {
     "duration": 1.451871,
     "end_time": "2021-06-21T14:15:36.562413",
     "exception": false,
     "start_time": "2021-06-21T14:15:35.110542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pretrainedmodels\n",
    "\n",
    "def get_model(pretrained):\n",
    "    if pretrained:\n",
    "        model = pretrainedmodels.__dict__[\"resnet18\"](\n",
    "                pretrained=\"imagenet\"\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        model = pretrainedmodels.__dict__[\"resnet18\"](\n",
    "                pretrined = None\n",
    "        )\n",
    "        \n",
    "    model.last_linear = nn.Sequential(\n",
    "                nn.BatchNorm1d(512),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.Linear(in_features = 512, out_features = 100),\n",
    "            nn.ReLU(),\n",
    "                nn.BatchNorm1d(100 ,eps=1e-05, momentum=0.1),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features = 100, out_features = 1 ),\n",
    "        nn.Sigmoid()\n",
    "       \n",
    "            )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-21T14:15:37.056207Z",
     "iopub.status.busy": "2021-06-21T14:15:37.054965Z",
     "iopub.status.idle": "2021-06-21T14:15:52.094166Z",
     "shell.execute_reply": "2021-06-21T14:15:52.093108Z",
     "shell.execute_reply.started": "2021-06-21T13:33:59.018924Z"
    },
    "papermill": {
     "duration": 15.098065,
     "end_time": "2021-06-21T14:15:52.094369",
     "exception": false,
     "start_time": "2021-06-21T14:15:36.996304",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfb860e11cae476cafebaa2730140823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import albumentations\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = 'cuda'\n",
    "epoch = 10\n",
    "targets = data.target.values\n",
    "image = data.image.values\n",
    "model = get_model(pretrained=True)\n",
    "model.to(device)\n",
    "\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "\n",
    "aug = albumentations.Compose(\n",
    "[\n",
    "    albumentations.Normalize(\n",
    "    mean, std, max_pixel_value=255.0, always_apply= True\n",
    "    )\n",
    "]\n",
    ")\n",
    "\n",
    "\n",
    "train_images, valid_images, train_targets, valid_targets = train_test_split(image, targets, stratify=targets, random_state=42)\n",
    "\n",
    "train_dataset = dataset(image_path= train_images, targets = train_targets, resize = (100,100), augmentation=aug)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader( train_dataset, batch_size = 16, shuffle = True, num_workers = 4)\n",
    "\n",
    "valid_dataset = dataset(image_path= valid_images, targets= valid_targets, resize= (100,100), augmentation= aug)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader( valid_dataset, batch_size = 16, shuffle = True, num_workers = 4)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 5e-4)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-21T14:15:52.163021Z",
     "iopub.status.busy": "2021-06-21T14:15:52.162390Z",
     "iopub.status.idle": "2021-06-21T15:22:58.699233Z",
     "shell.execute_reply": "2021-06-21T15:22:58.699805Z",
     "shell.execute_reply.started": "2021-06-21T13:38:53.034511Z"
    },
    "papermill": {
     "duration": 4026.57445,
     "end_time": "2021-06-21T15:22:58.699974",
     "exception": false,
     "start_time": "2021-06-21T14:15:52.125524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9685631024930748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9827594459833794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9840729085872576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.922558891966759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9741595567867036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9442958448753462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9694823268698061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9287115789473686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9585998891966758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9843423822714681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9833914681440443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9732314681440444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.948881108033241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9727290858725762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96019567867036\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    train(train_loader, model, optimizer, device = device)\n",
    "    \n",
    "    prediction, valid_targets = evaluation( valid_loader, model, device = device )\n",
    "    \n",
    "    roc_auc = metrics.roc_auc_score(valid_targets, prediction)\n",
    "    print(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-21T15:22:58.812694Z",
     "iopub.status.busy": "2021-06-21T15:22:58.811595Z",
     "iopub.status.idle": "2021-06-21T15:22:58.969959Z",
     "shell.execute_reply": "2021-06-21T15:22:58.969308Z",
     "shell.execute_reply.started": "2021-06-21T14:00:51.952823Z"
    },
    "papermill": {
     "duration": 0.217164,
     "end_time": "2021-06-21T15:22:58.970152",
     "exception": false,
     "start_time": "2021-06-21T15:22:58.752988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './model_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.055129,
     "end_time": "2021-06-21T15:22:59.080392",
     "exception": false,
     "start_time": "2021-06-21T15:22:59.025263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4076.352201,
   "end_time": "2021-06-21T15:23:01.511154",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-21T14:15:05.158953",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "241e4d63a7b04b1aac2dbefa65f59b67": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "293dd827c7fd41ac84c5c68ff93f9af5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "2f2d9670b47f499cad7e1aafe0d9ed66": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_241e4d63a7b04b1aac2dbefa65f59b67",
       "max": 46827520,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_293dd827c7fd41ac84c5c68ff93f9af5",
       "value": 46827520
      }
     },
     "3579b300225a426b956c15c879d4980b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "4388b78744df49ff8490733540ccebe4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8e7603bb020e4bcfaa13eb0e612679b2",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_3579b300225a426b956c15c879d4980b",
       "value": " 44.7M/44.7M [00:05&lt;00:00, 12.5MB/s]"
      }
     },
     "44ee85a7b89c4d7898aa2a885f66ff76": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "7096e910ba924ea9a151e838f63464d4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8e7603bb020e4bcfaa13eb0e612679b2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b6887d48d44e49ffbd6ff353b496220c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dfb860e11cae476cafebaa2730140823": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f37be04bc1a64b638a6ceb0f4f9af1bf",
        "IPY_MODEL_2f2d9670b47f499cad7e1aafe0d9ed66",
        "IPY_MODEL_4388b78744df49ff8490733540ccebe4"
       ],
       "layout": "IPY_MODEL_7096e910ba924ea9a151e838f63464d4"
      }
     },
     "f37be04bc1a64b638a6ceb0f4f9af1bf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b6887d48d44e49ffbd6ff353b496220c",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_44ee85a7b89c4d7898aa2a885f66ff76",
       "value": "100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
